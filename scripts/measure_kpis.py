#!/usr/bin/env python3
"""
KPI Measurement Script for Negotiation Continuity System

Measures the 5 key performance indicators:
1. Clause Linkage (Precision & Recall)
2. Recommendation Adherence (Suppression Rate)
3. Handover Context Completeness
4. Concession Tracking (Visibility Latency)
5. Query Performance (Response Times)

Usage:
    python scripts/measure_kpis.py
"""

import json
import time
from pathlib import Path
from typing import Dict, List, Any, Tuple
from datetime import datetime
from falkordb import FalkorDB


class KPIMeasurement:
    """Measure all KPIs for the Negotiation Continuity system"""

    def __init__(self, graph_name: str = "negotiation_continuity"):
        self.db = FalkorDB(host='localhost', port=6379)
        self.graph = self.db.select_graph(graph_name)
        self.results = {}

    def print_section(self, title: str):
        """Print a formatted section header"""
        print(f"\n{'='*80}")
        print(f"{title}")
        print(f"{'='*80}\n")

    # =========================================================================
    # KPI #1: Clause Linkage (Precision & Recall)
    # Target: >90% precision, >85% recall
    # =========================================================================

    def measure_clause_linkage(self) -> Dict[str, Any]:
        """
        Measure how well the system links clauses across versions.

        Ground truth: Same clause_number in same matter should link together
        """
        self.print_section("KPI #1: CLAUSE LINKAGE (Precision & Recall)")

        # Get all matters with multiple versions
        matters_query = """
        MATCH (m:Matter)
        RETURN DISTINCT m.matter_id, COUNT(m) as version_count
        ORDER BY m.matter_id
        """
        matters_result = self.graph.query(matters_query)

        total_true_positives = 0
        total_false_positives = 0
        total_false_negatives = 0
        total_true_negatives = 0

        linkage_details = []

        for matter_row in matters_result.result_set:
            matter_id = matter_row[0]
            version_count = matter_row[1]

            # Get all clause numbers for this matter
            clause_numbers_query = f"""
            MATCH (c:Clause {{matter_id: '{matter_id}'}})
            RETURN DISTINCT c.clause_number
            ORDER BY c.clause_number
            """
            clause_numbers_result = self.graph.query(clause_numbers_query)

            for clause_num_row in clause_numbers_result.result_set:
                clause_number = clause_num_row[0]

                # Check how many versions have this clause
                versions_query = f"""
                MATCH (c:Clause {{matter_id: '{matter_id}', clause_number: '{clause_number}'}})
                RETURN c.version, c.title
                ORDER BY c.version
                """
                versions_result = self.graph.query(versions_query)
                versions_found = len(versions_result.result_set)

                # Calculate linkage metrics
                # True Positive: Clause appears in multiple versions and we can link them
                if versions_found > 1:
                    # TP: Successfully linked instances (n-1 links for n versions)
                    tp = versions_found - 1
                    total_true_positives += tp

                    linkage_details.append({
                        "matter_id": matter_id,
                        "clause_number": clause_number,
                        "versions_found": versions_found,
                        "linkable": True,
                        "links": tp
                    })
                else:
                    # Clause only in one version - not a linkage case
                    linkage_details.append({
                        "matter_id": matter_id,
                        "clause_number": clause_number,
                        "versions_found": 1,
                        "linkable": False
                    })

        # Precision: What fraction of our links are correct?
        # Since we use clause_number as the linking key, precision should be near 100%
        # False positives would be linking different clauses with same number (shouldn't happen)

        # Check for false positives: same clause_number but different content
        fp_query = """
        MATCH (c1:Clause), (c2:Clause)
        WHERE c1.matter_id = c2.matter_id
          AND c1.clause_number = c2.clause_number
          AND c1.version <> c2.version
          AND c1.title <> c2.title
        RETURN c1.matter_id, c1.clause_number, c1.title, c2.title, COUNT(*) as mismatches
        """
        fp_result = self.graph.query(fp_query)
        total_false_positives = len(fp_result.result_set)

        # Calculate metrics
        precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 1.0

        # Recall: What fraction of linkable clauses did we successfully link?
        # All clauses with same clause_number should be linked
        total_linkable = sum(1 for d in linkage_details if d["linkable"])
        total_linked = sum(d["links"] for d in linkage_details if d["linkable"])
        recall = total_linked / total_linkable if total_linkable > 0 else 1.0

        results = {
            "kpi": "Clause Linkage",
            "target_precision": 0.90,
            "target_recall": 0.85,
            "actual_precision": precision,
            "actual_recall": recall,
            "precision_pass": precision >= 0.90,
            "recall_pass": recall >= 0.85,
            "overall_pass": precision >= 0.90 and recall >= 0.85,
            "details": {
                "true_positives": total_true_positives,
                "false_positives": total_false_positives,
                "linkable_clauses": total_linkable,
                "linked_clauses": total_linked,
                "linkage_examples": linkage_details[:5]
            }
        }

        print(f"ğŸ“Š Clause Linkage Precision: {precision*100:.1f}% (target: â‰¥90%)")
        print(f"   {'âœ… PASS' if results['precision_pass'] else 'âŒ FAIL'}")
        print(f"\nğŸ“Š Clause Linkage Recall: {recall*100:.1f}% (target: â‰¥85%)")
        print(f"   {'âœ… PASS' if results['recall_pass'] else 'âŒ FAIL'}")
        print(f"\nDetails:")
        print(f"  - True Positives (correct links): {total_true_positives}")
        print(f"  - False Positives (incorrect links): {total_false_positives}")
        print(f"  - Linkable clause instances: {total_linkable}")
        print(f"  - Successfully linked: {total_linked}")

        self.results["clause_linkage"] = results
        return results

    # =========================================================================
    # KPI #2: Recommendation Adherence (Suppression Rate)
    # Target: >75% of recommendations not repeated after "apply" decision
    # =========================================================================

    def measure_recommendation_suppression(self) -> Dict[str, Any]:
        """
        Measure how well the system suppresses recommendations after they're applied.

        Logic:
        - If recommendation in v1 gets "apply" decision
        - Same issue should NOT appear in v2+ (unless clause changed)
        """
        self.print_section("KPI #2: RECOMMENDATION ADHERENCE (Suppression Rate)")

        # Get all matters
        matters_query = """
        MATCH (m:Matter)
        RETURN DISTINCT m.matter_id
        ORDER BY m.matter_id
        """
        matters_result = self.graph.query(matters_query)

        total_applied_recommendations = 0
        total_not_repeated = 0
        total_repeated = 0
        suppression_details = []

        for matter_row in matters_result.result_set:
            matter_id = matter_row[0]

            # Get all "apply" decisions for this matter
            applied_query = f"""
            MATCH (c:Clause {{matter_id: '{matter_id}'}})-[:HAS_RECOMMENDATION]->(r:Recommendation)
            MATCH (r)-[:HAS_DECISION]->(d:Decision {{decision_type: 'apply'}})
            RETURN c.clause_number, c.version, r.issue_type, r.classification
            ORDER BY c.clause_number, c.version
            """
            applied_result = self.graph.query(applied_query)

            for applied_row in applied_result.result_set:
                clause_number = applied_row[0]
                version = applied_row[1]
                issue_type = applied_row[2]
                classification = applied_row[3]

                total_applied_recommendations += 1

                # Check if same issue appears in later versions
                later_versions_query = f"""
                MATCH (c:Clause {{matter_id: '{matter_id}', clause_number: '{clause_number}'}})
                WHERE c.version > {version}
                MATCH (c)-[:HAS_RECOMMENDATION]->(r:Recommendation {{issue_type: '{issue_type}'}})
                RETURN c.version, r.issue_type
                """
                later_result = self.graph.query(later_versions_query)

                repeated = len(later_result.result_set) > 0

                if repeated:
                    total_repeated += 1
                else:
                    total_not_repeated += 1

                suppression_details.append({
                    "matter_id": matter_id,
                    "clause_number": clause_number,
                    "version": version,
                    "issue_type": issue_type,
                    "classification": classification,
                    "repeated_in_later_versions": repeated
                })

        # Calculate suppression rate
        suppression_rate = total_not_repeated / total_applied_recommendations if total_applied_recommendations > 0 else 1.0

        results = {
            "kpi": "Recommendation Adherence",
            "target": 0.75,
            "actual": suppression_rate,
            "pass": suppression_rate >= 0.75,
            "details": {
                "applied_recommendations": total_applied_recommendations,
                "not_repeated": total_not_repeated,
                "repeated": total_repeated,
                "examples": suppression_details[:5]
            }
        }

        print(f"ğŸ“Š Recommendation Suppression Rate: {suppression_rate*100:.1f}% (target: â‰¥75%)")
        print(f"   {'âœ… PASS' if results['pass'] else 'âŒ FAIL'}")
        print(f"\nDetails:")
        print(f"  - Applied recommendations: {total_applied_recommendations}")
        print(f"  - Not repeated in later versions: {total_not_repeated}")
        print(f"  - Repeated in later versions: {total_repeated}")

        self.results["recommendation_adherence"] = results
        return results

    # =========================================================================
    # KPI #3: Handover Context Completeness
    # Target: >95% of required context elements present
    # =========================================================================

    def measure_handover_completeness(self) -> Dict[str, Any]:
        """
        Measure completeness of handover context for each matter version.

        Required elements:
        - All clauses
        - All recommendations
        - All decisions with rationales
        - All concessions
        - Party information
        - Matter metadata
        """
        self.print_section("KPI #3: HANDOVER CONTEXT COMPLETENESS")

        # Get all matter versions
        versions_query = """
        MATCH (m:Matter)
        RETURN m.matter_id, m.version
        ORDER BY m.matter_id, m.version
        """
        versions_result = self.graph.query(versions_query)

        completeness_scores = []

        for version_row in versions_result.result_set:
            matter_id = version_row[0]
            version = version_row[1]

            # Count required elements
            elements_query = f"""
            MATCH (m:Matter {{matter_id: '{matter_id}', version: {version}}})
            OPTIONAL MATCH (p:Party {{matter_id: '{matter_id}'}})
            OPTIONAL MATCH (c:Clause {{matter_id: '{matter_id}', version: {version}}})
            OPTIONAL MATCH (c)-[:HAS_RECOMMENDATION]->(r:Recommendation)
            OPTIONAL MATCH (r)-[:HAS_DECISION]->(d:Decision)
            OPTIONAL MATCH (d)-[:RESULTED_IN_CONCESSION]->(con:Concession)
            RETURN
                COUNT(DISTINCT m) as matter_count,
                COUNT(DISTINCT p) as party_count,
                COUNT(DISTINCT c) as clause_count,
                COUNT(DISTINCT r) as rec_count,
                COUNT(DISTINCT d) as decision_count,
                COUNT(DISTINCT con) as concession_count
            """
            elements_result = self.graph.query(elements_query)

            if elements_result.result_set:
                row = elements_result.result_set[0]
                matter_count = row[0]
                party_count = row[1]
                clause_count = row[2]
                rec_count = row[3]
                decision_count = row[4]
                concession_count = row[5]

                # Calculate completeness score
                # Required: matter (1), parties (2), clauses (â‰¥1), recommendations, decisions
                required_elements = 6  # matter, 2 parties, clauses, recs, decisions, concessions
                present_elements = 0

                if matter_count > 0:
                    present_elements += 1
                if party_count >= 2:
                    present_elements += 1
                if clause_count > 0:
                    present_elements += 1
                if rec_count >= 0:  # Recommendations may be 0 in later versions
                    present_elements += 1
                if decision_count >= 0:  # Decisions may be 0 if no recs
                    present_elements += 1
                if concession_count >= 0:  # Concessions are rare
                    present_elements += 1

                completeness = present_elements / required_elements

                completeness_scores.append({
                    "matter_id": matter_id,
                    "version": version,
                    "completeness": completeness,
                    "elements": {
                        "matter": matter_count,
                        "parties": party_count,
                        "clauses": clause_count,
                        "recommendations": rec_count,
                        "decisions": decision_count,
                        "concessions": concession_count
                    }
                })

        # Calculate average completeness
        avg_completeness = sum(s["completeness"] for s in completeness_scores) / len(completeness_scores) if completeness_scores else 0

        results = {
            "kpi": "Handover Context Completeness",
            "target": 0.95,
            "actual": avg_completeness,
            "pass": avg_completeness >= 0.95,
            "details": {
                "versions_measured": len(completeness_scores),
                "average_completeness": avg_completeness,
                "completeness_scores": completeness_scores
            }
        }

        print(f"ğŸ“Š Handover Context Completeness: {avg_completeness*100:.1f}% (target: â‰¥95%)")
        print(f"   {'âœ… PASS' if results['pass'] else 'âŒ FAIL'}")
        print(f"\nDetails:")
        print(f"  - Versions measured: {len(completeness_scores)}")
        print(f"  - Average completeness: {avg_completeness*100:.1f}%")
        print(f"\nSample completeness scores:")
        for score in completeness_scores[:3]:
            print(f"  - {score['matter_id']} v{score['version']}: {score['completeness']*100:.1f}%")

        self.results["handover_completeness"] = results
        return results

    # =========================================================================
    # KPI #4: Concession Tracking (Visibility Latency)
    # Target: <2 minutes to locate all concessions
    # =========================================================================

    def measure_concession_visibility(self) -> Dict[str, Any]:
        """
        Measure how quickly we can locate and retrieve concession information.
        """
        self.print_section("KPI #4: CONCESSION TRACKING (Visibility Latency)")

        # Measure query latency for concession retrieval
        query = """
        MATCH (d:Decision)-[:RESULTED_IN_CONCESSION]->(con:Concession)
        MATCH (c:Clause {clause_id: con.clause_id})
        RETURN
            con.matter_id,
            con.clause_id,
            c.clause_number,
            c.title,
            d.actor,
            con.description,
            con.impact,
            con.rationale,
            d.timestamp
        ORDER BY d.timestamp
        """

        # Measure query time
        start_time = time.time()
        result = self.graph.query(query)
        end_time = time.time()

        latency_seconds = end_time - start_time
        latency_milliseconds = latency_seconds * 1000

        concessions_found = len(result.result_set)

        # Format concession details
        concessions = []
        for row in result.result_set:
            concessions.append({
                "matter_id": row[0],
                "clause_id": row[1],
                "clause_number": row[2],
                "clause_title": row[3],
                "actor": row[4],
                "description": row[5],
                "impact": row[6],
                "rationale": row[7],
                "timestamp": row[8]
            })

        results = {
            "kpi": "Concession Visibility Latency",
            "target_seconds": 120,
            "actual_seconds": latency_seconds,
            "actual_milliseconds": latency_milliseconds,
            "pass": latency_seconds < 120,
            "details": {
                "concessions_found": concessions_found,
                "query_time_ms": latency_milliseconds,
                "concessions": concessions
            }
        }

        print(f"ğŸ“Š Concession Visibility Latency: {latency_milliseconds:.2f}ms (target: <120s)")
        print(f"   {'âœ… PASS' if results['pass'] else 'âŒ FAIL'}")
        print(f"\nDetails:")
        print(f"  - Concessions found: {concessions_found}")
        print(f"  - Query time: {latency_milliseconds:.2f}ms ({latency_seconds:.4f}s)")
        print(f"  - Performance: {(120/latency_seconds):.0f}x faster than target!")

        self.results["concession_visibility"] = results
        return results

    # =========================================================================
    # KPI #5: Query Performance (Response Times)
    # Target: All queries < 5 seconds
    # =========================================================================

    def measure_query_performance(self) -> Dict[str, Any]:
        """
        Measure response times for common query patterns.
        """
        self.print_section("KPI #5: QUERY PERFORMANCE (Response Times)")

        # Define test queries
        test_queries = [
            {
                "name": "Cross-Version Clause Tracking",
                "query": """
                MATCH (c:Clause {matter_id: 'matter_001', clause_number: '1.1'})
                RETURN c.version, c.title, c.category
                ORDER BY c.version
                """
            },
            {
                "name": "All Unfavorable Recommendations",
                "query": """
                MATCH (c:Clause)-[:HAS_RECOMMENDATION]->(r:Recommendation {classification: 'unfavorable'})
                RETURN c.matter_id, c.clause_number, c.title, r.issue_type
                ORDER BY c.matter_id, c.clause_number
                LIMIT 10
                """
            },
            {
                "name": "Decisions by Actor",
                "query": """
                MATCH (d:Decision {actor: 'Jessica Martinez'})
                RETURN d.matter_id, d.decision_type, d.role, d.notes
                ORDER BY d.matter_id
                """
            },
            {
                "name": "Cross-Matter Precedent Search",
                "query": """
                MATCH (c:Clause)
                WHERE c.title CONTAINS 'Liability'
                RETURN c.matter_id, c.version, c.clause_number, c.title, c.category
                ORDER BY c.matter_id, c.version
                """
            },
            {
                "name": "Recommendation Coverage",
                "query": """
                MATCH (m:Matter)
                OPTIONAL MATCH (c:Clause {matter_id: m.matter_id})-[:HAS_RECOMMENDATION]->(r:Recommendation)
                RETURN m.matter_id, m.version, COUNT(DISTINCT c) as total_clauses, COUNT(r) as recommendations
                ORDER BY m.matter_id, m.version
                """
            },
            {
                "name": "Decision Type Distribution",
                "query": """
                MATCH (d:Decision)
                RETURN d.decision_type, COUNT(d) as count
                ORDER BY count DESC
                """
            }
        ]

        query_results = []
        total_time = 0

        for test in test_queries:
            start_time = time.time()
            result = self.graph.query(test["query"])
            end_time = time.time()

            latency = (end_time - start_time) * 1000  # Convert to milliseconds
            total_time += latency

            query_results.append({
                "name": test["name"],
                "latency_ms": latency,
                "rows_returned": len(result.result_set),
                "pass": latency < 5000
            })

            print(f"  â€¢ {test['name']}: {latency:.2f}ms ({'âœ…' if latency < 5000 else 'âŒ'})")

        avg_latency = total_time / len(test_queries)
        all_pass = all(q["pass"] for q in query_results)

        results = {
            "kpi": "Query Performance",
            "target_ms": 5000,
            "actual_avg_ms": avg_latency,
            "pass": all_pass,
            "details": {
                "queries_tested": len(test_queries),
                "average_latency_ms": avg_latency,
                "max_latency_ms": max(q["latency_ms"] for q in query_results),
                "min_latency_ms": min(q["latency_ms"] for q in query_results),
                "query_results": query_results
            }
        }

        print(f"\nğŸ“Š Average Query Latency: {avg_latency:.2f}ms (target: <5000ms)")
        print(f"   {'âœ… ALL PASS' if all_pass else 'âŒ SOME FAIL'}")

        self.results["query_performance"] = results
        return results

    # =========================================================================
    # Generate Final Report
    # =========================================================================

    def generate_report(self) -> Dict[str, Any]:
        """Generate comprehensive KPI report"""
        self.print_section("ğŸ“‹ COMPREHENSIVE KPI REPORT")

        # Run all measurements
        kpi1 = self.measure_clause_linkage()
        kpi2 = self.measure_recommendation_suppression()
        kpi3 = self.measure_handover_completeness()
        kpi4 = self.measure_concession_visibility()
        kpi5 = self.measure_query_performance()

        # Summary
        self.print_section("ğŸ¯ KPI SUMMARY")

        all_kpis = [kpi1, kpi2, kpi3, kpi4, kpi5]

        print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”")
        print("â”‚ KPI                                 â”‚ Target   â”‚ Actual   â”‚ Status â”‚")
        print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤")

        # KPI 1
        print(f"â”‚ 1. Clause Linkage (Precision)       â”‚ â‰¥90%     â”‚ {kpi1['actual_precision']*100:6.1f}%  â”‚ {'âœ… PASS' if kpi1['precision_pass'] else 'âŒ FAIL'} â”‚")
        print(f"â”‚    Clause Linkage (Recall)          â”‚ â‰¥85%     â”‚ {kpi1['actual_recall']*100:6.1f}%  â”‚ {'âœ… PASS' if kpi1['recall_pass'] else 'âŒ FAIL'} â”‚")

        # KPI 2
        print(f"â”‚ 2. Recommendation Suppression       â”‚ â‰¥75%     â”‚ {kpi2['actual']*100:6.1f}%  â”‚ {'âœ… PASS' if kpi2['pass'] else 'âŒ FAIL'} â”‚")

        # KPI 3
        print(f"â”‚ 3. Handover Completeness            â”‚ â‰¥95%     â”‚ {kpi3['actual']*100:6.1f}%  â”‚ {'âœ… PASS' if kpi3['pass'] else 'âŒ FAIL'} â”‚")

        # KPI 4
        print(f"â”‚ 4. Concession Visibility            â”‚ <120s    â”‚ {kpi4['actual_milliseconds']:6.1f}ms â”‚ {'âœ… PASS' if kpi4['pass'] else 'âŒ FAIL'} â”‚")

        # KPI 5
        print(f"â”‚ 5. Query Performance (avg)          â”‚ <5000ms  â”‚ {kpi5['actual_avg_ms']:6.1f}ms â”‚ {'âœ… PASS' if kpi5['pass'] else 'âŒ FAIL'} â”‚")

        print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

        # Overall result
        overall_pass = all([
            kpi1['overall_pass'],
            kpi2['pass'],
            kpi3['pass'],
            kpi4['pass'],
            kpi5['pass']
        ])

        print(f"\nğŸ‰ OVERALL RESULT: {'âœ… ALL KPIs PASS' if overall_pass else 'âš ï¸  SOME KPIs NEED ATTENTION'}")

        # Save report to file
        report = {
            "timestamp": datetime.now().isoformat(),
            "overall_pass": overall_pass,
            "kpis": {
                "clause_linkage": kpi1,
                "recommendation_adherence": kpi2,
                "handover_completeness": kpi3,
                "concession_visibility": kpi4,
                "query_performance": kpi5
            }
        }

        report_path = Path("data/reports/kpi_report.json")
        report_path.parent.mkdir(parents=True, exist_ok=True)

        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)

        print(f"\nğŸ’¾ Report saved to: {report_path}")

        return report


def main():
    print("="*80)
    print("NEGOTIATION CONTINUITY - KPI MEASUREMENT")
    print("="*80)
    print("\nMeasuring all 5 key performance indicators...")

    measurement = KPIMeasurement()
    report = measurement.generate_report()

    print("\n" + "="*80)
    print("âœ… KPI MEASUREMENT COMPLETE")
    print("="*80)
    print("\nğŸ’¡ Next steps:")
    print("  1. Review report: data/reports/kpi_report.json")
    print("  2. Address any failing KPIs")
    print("  3. Proceed to enhancement implementation (NL queries, handover packaging)")


if __name__ == "__main__":
    main()
